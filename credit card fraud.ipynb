{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "     \n",
    "\n",
    "# Importing dataset\n",
    "cred_data=pd.read_csv('https://raw.githubusercontent.com/sautrikc/-Credit-Card-Fraud-Detection/main/UCI_Credit_Card.csv')\n",
    "     \n",
    "\n",
    "cred_data.head()\n",
    "cred_data.shape\n",
    "cred_data.info()\n",
    "# Category 0,5,6 are undocumented so needed to be checked\n",
    "print(cred_data['EDUCATION'].value_counts())\n",
    "sns.countplot(cred_data['EDUCATION'])\n",
    "print(cred_data['MARRIAGE'].value_counts())\n",
    "sns.countplot(cred_data['MARRIAGE'])\n",
    "\n",
    "# Some of the age values are more than 70 which is fine.\n",
    "cred_data['AGE'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba570",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6871fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the pay are having -2,0 category that are undocumented.\n",
    "# Given category -1 as pay duly(properly paid on time)\n",
    "# So -2,0,-1 can be treated as one category. to be checked\n",
    "print(cred_data['PAY_0'].value_counts())\n",
    "print(cred_data['PAY_2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BILL_AMT1 having 1% of negative values\n",
    "# BILL_AMT2 TO BILL_AMT6 having 2% negative values\n",
    "# can be treated as extra paid\n",
    "\n",
    "# Looks like Bill amount has 1% of outliers - to be cleaned\n",
    "cred_data[['BILL_AMT1','BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].quantile([0,0.01,0.02,0.03,0.04,0.05,0.06,0.08,0.09,0.1,\n",
    "0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.93,0.94,0.97,0.99,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_data[['BILL_AMT1','BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].plot(kind='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay amount has 1% of extreme outlier needed to be cleaned\n",
    "cred_data[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].quantile([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.93,0.94,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Outliers have completely compressed the boxplot\n",
    "cred_data[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a160bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit_Bal contains 1% of extreme outliers - require cleaning\n",
    "cred_data['LIMIT_BAL'].quantile([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.93,0.94,0.97,0.99,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=cred_data['LIMIT_BAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply Category 5,6,0 values are undocumented so we can pour it into category 4 metioned others.\n",
    "unknown=(cred_data['EDUCATION']==5)|(cred_data['EDUCATION']==6)|(cred_data['EDUCATION']==0)\n",
    "cred_data.loc[unknown,'EDUCATION']=4\n",
    "cred_data['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 0 value are undocumented so we can pour it into category 3 mentioned as others.\n",
    "unknown=cred_data['MARRIAGE']==0\n",
    "cred_data.loc[unknown,'MARRIAGE']=3\n",
    "cred_data['MARRIAGE'].value_counts()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76597bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of each category\n",
    "cred_data.PAY_0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross tab with target\n",
    "cross_cred_data2=pd.crosstab(cred_data['PAY_2'],cred_data['default.payment.next.month'])\n",
    "     \n",
    "\n",
    "#Cross tab row Percentages\n",
    "cross_cred_data2_percent=cross_cred_data2.apply(lambda x: x/x.sum(), axis=1)\n",
    "round(cross_cred_data2_percent,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae92073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating category -2,-1,0 value as one category 0 for pay duly \n",
    "# so going for imputation based on target variable values nearest to them.\n",
    "fil = (cred_data.PAY_0 == -2) | (cred_data.PAY_0 == -1) | (cred_data.PAY_0 == 0)\n",
    "cred_data.loc[fil, 'PAY_0'] = 0\n",
    "fil = (cred_data.PAY_2 == -2) | (cred_data.PAY_2 == -1) | (cred_data.PAY_2 == 0)\n",
    "cred_data.loc[fil, 'PAY_2'] = 0\n",
    "fil = (cred_data.PAY_3 == -2) | (cred_data.PAY_3 == -1) | (cred_data.PAY_3 == 0)\n",
    "cred_data.loc[fil, 'PAY_3'] = 0\n",
    "fil = (cred_data.PAY_4 == -2) | (cred_data.PAY_4 == -1) | (cred_data.PAY_4 == 0)\n",
    "cred_data.loc[fil, 'PAY_4'] = 0\n",
    "fil = (cred_data.PAY_5 == -2) | (cred_data.PAY_5 == -1) | (cred_data.PAY_5 == 0)\n",
    "cred_data.loc[fil, 'PAY_5'] = 0\n",
    "fil = (cred_data.PAY_6 == -2) | (cred_data.PAY_6 == -1) | (cred_data.PAY_6 == 0)\n",
    "cred_data.loc[fil, 'PAY_6'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553452a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Pay_0 as pay_1 and default.payment.next as def_pay\n",
    "cred_data.rename(columns={'PAY_0':'PAY_1','default.payment.next.month':'DEF_PAY'},inplace=True)\n",
    "pd.set_option('max_columns',None)\n",
    "cred_data.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_data[['BILL_AMT1','BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].quantile([0,0.01,0.02,0.03,0.04,0.05,0.08,0.09,0.1,0.2,0.3,0.4,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating 1% positive outlier by imputation based on the median value of that column\n",
    "cred_data.loc[cred_data['BILL_AMT1']>350110.68,'BILL_AMT1']=cred_data['BILL_AMT1'].median()\n",
    "cred_data.loc[cred_data['BILL_AMT2']>337495.28,'BILL_AMT2']=cred_data['BILL_AMT2'].median()\n",
    "cred_data.loc[cred_data['BILL_AMT3']>325030.39,'BILL_AMT3']=cred_data['BILL_AMT3'].median()\n",
    "cred_data.loc[cred_data['BILL_AMT4']>304997.27,'BILL_AMT4']=cred_data['BILL_AMT4'].median()\n",
    "cred_data.loc[cred_data['BILL_AMT5']>285868.33,'BILL_AMT5']=cred_data['BILL_AMT5'].median()\n",
    "cred_data.loc[cred_data['BILL_AMT6']>279505.06,'BILL_AMT6']=cred_data['BILL_AMT6'].median()\n",
    "     \n",
    "\n",
    "cred_data[['BILL_AMT1','BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating 1% negative outlier in BILL_AMT1 and 2% negative outlier in the rest BILL_AMT\n",
    "# by imputation based on the median value of the negative values of the columns\n",
    "cred_data.loc[cred_data['BILL_AMT1']<0,'BILL_AMT1']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "cred_data.loc[cred_data['BILL_AMT2']<0,'BILL_AMT2']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "cred_data.loc[cred_data['BILL_AMT3']<0,'BILL_AMT3']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "cred_data.loc[cred_data['BILL_AMT4']<0,'BILL_AMT4']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "cred_data.loc[cred_data['BILL_AMT5']<0,'BILL_AMT5']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "cred_data.loc[cred_data['BILL_AMT6']<0,'BILL_AMT6']=cred_data['BILL_AMT1'][cred_data['BILL_AMT1']<0].median()\n",
    "     \n",
    "\n",
    "cred_data[['BILL_AMT1','BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating 1% extreme outlier by imputation based on the median value of that column\n",
    "cred_data.loc[cred_data['PAY_AMT1']>67000,'PAY_AMT1']=cred_data['PAY_AMT1'].median()\n",
    "cred_data.loc[cred_data['PAY_AMT2']>76700,'PAY_AMT2']=cred_data['PAY_AMT2'].median()\n",
    "cred_data.loc[cred_data['PAY_AMT3']>70000,'PAY_AMT3']=cred_data['PAY_AMT3'].median()\n",
    "cred_data.loc[cred_data['PAY_AMT4']>67100,'PAY_AMT4']=cred_data['PAY_AMT4'].median()\n",
    "cred_data.loc[cred_data['PAY_AMT5']>65700,'PAY_AMT5']=cred_data['PAY_AMT5'].median()\n",
    "cred_data.loc[cred_data['PAY_AMT6']>82700,'PAY_AMT6']=cred_data['PAY_AMT6'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df435688",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_data[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].plot(kind='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating 1% extreme outlier by imputation based on the median value of that column\n",
    "cred_data.loc[cred_data['LIMIT_BAL']>500000,'LIMIT_BAL']=cred_data['LIMIT_BAL'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbaece",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=cred_data['LIMIT_BAL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b21a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the clean data for further analysis.\n",
    "cred_data_new=cred_data.copy(deep=True)\n",
    "cred_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X array that will contain features and y array will contain the target vector\n",
    "X=cred_data_new.drop('DEF_PAY',axis=1)\n",
    "y=cred_data_new['DEF_PAY']\n",
    "\n",
    "# Importing the package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using train_test_split() function to split the whole data to train data of 80% and test data of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Checking the shapes of the train and test data\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c44e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic=LogisticRegression(max_iter=2000)\n",
    "\n",
    "\n",
    "# Building a Multiple Logistic Regression Model by fitting the target and the features\n",
    "logistic.fit(X_train,y_train)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb48f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient and intercept\n",
    "print(logistic.coef_)\n",
    "print(logistic.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of target using the features\n",
    "pred=logistic.predict(X_test)\n",
    "pred\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5421af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the sklearn package for creating the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,pred)\n",
    "cm\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9329d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the model\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "round(accuracy*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c31dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the package\n",
    "import statsmodels.formula.api as sm\n",
    "# Creating function for calculating VIF\n",
    "def vif_cal(input_data):\n",
    "    x_vars = input_data\n",
    "    xvar_names=x_vars.columns\n",
    "    for i in range(0,xvar_names.shape[0]):\n",
    "        y=x_vars[xvar_names[i]] \n",
    "        x=x_vars[xvar_names.drop(xvar_names[i])]\n",
    "        rsq=sm.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n",
    "        vif=round(1/(1-rsq),2)\n",
    "        print (xvar_names[i], \" VIF = \" , vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating VIF for all the Features\n",
    "vif_cal(input_data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping squentially the feature having the high vif\n",
    "vif_cal(input_data=X_train.drop('BILL_AMT2',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91858e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_cal(input_data=X_train.drop(['BILL_AMT2','BILL_AMT5'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ef382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_cal(input_data=X_train.drop(['BILL_AMT2','BILL_AMT5','BILL_AMT3'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are no more high vif that means no more interdependency between the feature\n",
    "vif_cal(input_data=X_train.drop(['BILL_AMT2','BILL_AMT5','BILL_AMT3','BILL_AMT4'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the statsmodel library\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "m=sm.Logit(y,X)\n",
    "# Fitting feature to the model\n",
    "Res=m.fit()\n",
    "# Printing Summary\n",
    "print(Res.summary())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the feature that are not required\n",
    "X.drop(['BILL_AMT1', 'BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6','PAY_2','ID'],axis=1,inplace=True)\n",
    "     \n",
    "\n",
    "# Importing the statsmodel library\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "m=sm.Logit(y,X)\n",
    "# Fitting feature to the model\n",
    "Res=m.fit()\n",
    "# Printing Summary\n",
    "print(Res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a665e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_test_split() function to split the whole data to train data of 80% and test data of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Checking the shapes of the train and test data\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the feature and target vector to the model\n",
    "logistic.fit(X_train,y_train)\n",
    "# Predicting the target\n",
    "pred=logistic.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "cm=confusion_matrix(y_test,pred)\n",
    "cm\n",
    "# Calculating Accuracy\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Accuracy=',round(accuracy*100,3))\n",
    "\n",
    "# Calculating Sensitivity\n",
    "Sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity-',round(Sensitivity*100,2))\n",
    "\n",
    "# Calculating Specificity\n",
    "Specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity-',round(Specificity*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3594a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(Res.tvalues.pow(2)).sort_values(ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=cred_data_new[['PAY_1','MARRIAGE','LIMIT_BAL','SEX','EDUCATION']]\n",
    "y1=cred_data_new['DEF_PAY']\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=50)\n",
    "\n",
    "# Fitting the target and the features\n",
    "logistic.fit(X1_train,y1_train)\n",
    "\n",
    "#predict\n",
    "Pred2=logistic.predict(X1_test)\n",
    "Pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Accuracy\n",
    "cm1=confusion_matrix(y1_test,Pred2)\n",
    "print(cm1)\n",
    "\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Accuracy=',round(accuracy*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the clean data for further analysis.\n",
    "cred_data_new=cred_data.copy(deep=True)\n",
    "cred_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4554c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cred_data_new.drop('DEF_PAY',axis=1)\n",
    "y=cred_data_new['DEF_PAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the statsmodel library\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "m=sm.Logit(y,X)\n",
    "# Fitting feature to the model\n",
    "results=m.fit()\n",
    "# Printing Summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99085c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the traget using the features\n",
    "predict1=results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab723b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Taking the threshold value 0.5 as it is logistic regression\n",
    "threshold=0.5\n",
    "predictions1=[ 0 if x < threshold else 1 for x in predict1]\n",
    "\n",
    "# Confusion Matrix and Accuracy\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=confusion_matrix(y,predictions1)\n",
    "print(cm)\n",
    "\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Accuracy=',round(accuracy*100,2))\n",
    "\n",
    "# Calculating Sensitivity\n",
    "Sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity-',round(Sensitivity*100,2))\n",
    "\n",
    "# Calculating Specificity\n",
    "Specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity-',round(Specificity*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b28fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X array that will contain features and y array will contain the target vector\n",
    "X=cred_data_new.drop('DEF_PAY',axis=1)\n",
    "y=cred_data_new['DEF_PAY']\n",
    "\n",
    "# Importing the package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using train_test_split() function to split the whole data to train data of 80% and test data of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Importing the Library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic=LogisticRegression( solver='newton-cg', max_iter=200)\n",
    "\n",
    "# Building a Multiple Logistic Regression Model by fitting the target and the features\n",
    "logistic.fit(X_train,y_train)\n",
    "\n",
    "# Prediction of target using the features\n",
    "pred=logistic.predict(X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Accuracy\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=confusion_matrix(y_train,pred)\n",
    "print(cm)\n",
    "\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Accuracy=',round(accuracy*100,2))\n",
    "\n",
    "# Calculating Sensitivity\n",
    "Sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity-',round(Sensitivity*100,2))\n",
    "\n",
    "# Calculating Specificity\n",
    "Specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity-',round(Specificity*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cred_new_smote.drop('DEF_PAY',axis=1)\n",
    "y=cred_new_smote['DEF_PAY']\n",
    "\n",
    "# Importing the package\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Building XGB model for binary classification\n",
    "model_new=XGBClassifier(n_estimators=100,max_depth=5,learning_rate=0.1,eval_metric='error',\n",
    "                        early_stopping_rounds=4,tree_method='hist')\n",
    "\n",
    "model_new.fit(X_train,y_train)\n",
    "\n",
    "predict2=model_new.predict(X_train)\n",
    "\n",
    "print(\"Time taken by XGB \"+ str((time.time() - start_time))+ \" Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Creating Confusion Matrix using the train data and the predicted value\n",
    "cm=confusion_matrix(y_train,predict2)\n",
    "print(cm)\n",
    "\n",
    "# Calculating the accuracy of the predicted value of the test data\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Accuracy=',round(accuracy*100,2))\n",
    "\n",
    "# Calculating Sensitivity\n",
    "Sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity-',round(Sensitivity*100,2))\n",
    "\n",
    "# Calculating Specificity\n",
    "Specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity-',round(Specificity*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e50cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the target using the train data features\n",
    "predict3=model_new.predict(X_test)\n",
    "\n",
    "# Creating Confusion matrix using the test data and the predicted value\n",
    "cm1=confusion_matrix(y_test,predict3)\n",
    "print(cm1)\n",
    "\n",
    "# Calculating the accuracy of the train data\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print('Train Accuracy=',round(accuracy*100,2))\n",
    "\n",
    "# Calculating the accuracy of the test data\n",
    "total1=sum(sum(cm1))\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print('Test Accuracy=',round(accuracy1*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73367eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for Checking Recall Precision and F1-Score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train,predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b9501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdb6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
